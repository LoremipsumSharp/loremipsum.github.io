[{"title":"使用Utf8Json改善Ocelot对象序列化的性能","url":"/2021/04/07/使用Utf8Json改善Ocelot对象序列化的性能/","content":"\n## 背景\n\n近期在对部门的crm系统进行服务化改造，服务化改造过程中，选择了ocelot作为api网关。各个服务的response到达ocelot之后，ocelot会对response做进一步封装，形成统一的规范格式后再返回给client，eg：\n\n\n```\n\n    public class Response\n    {\n        public Response(object data)\n        {\n            Code = 0;\n            Data = data;\n            Message = \"Success\";\n        }\n        public Response(string message, int code = 500)\n        {\n            Code = code;\n            Message = message;\n        }\n        public int Code { get; set; }\n\n        public string Message { get; set; }\n\n        public object Data { get; set; } // 服务端返回的数据\n\n        public bool Success { get { return Code == 0; } }\n\n        public string Error {get;set;}\n```\n\n通过ocelot对response格式进行统一处理，这样依赖就不需要每个服务再去单独写一个ResultFilter来处理，减少了大量重复代码。\n\n为了实现这个功能，那么就必须在ocelot中对各个服务的response进行序列化/反序列化处理。这样势必对ocelot的性能造成一定的影响。\n那么有没有办法优化序列化/反序列化的性能呢？\n\n\n## Utf8Json\n\n[Utf8Json](https://github.com/neuecc/Utf8Json) 是一个日本人写的一个Json序列化器，目前宣称是拥有最好的序列化性能。相关[结论](https://michaelscodingspot.com/the-battle-of-c-to-json-serializers-in-net-core-3/)也支持这一观点:\n\n**For serialization, Utf8Json is 2 times faster than System.Text.Json and a whole 4 times faster than Newtonsoft. For deserialization, Utf8Json is 3.5 times faster than System.Text.Json and 6 times faster than Newtonsoft.**\n\n\n查阅官方文档，utf8json之所以这么快主要原因有如下几个：\n\n1. 不需要额外的内存分配\n以Json.NET为例，如果我们希望将一个对象变成一个字节数组,待会会有以下两种方法：\n\n方法一：\n```\npublic static byte[] SerializeJson(DrawDescriptionLayer layer)\n{\n    var s = JsonConvert.SerializeObject(layer, js);\n    return Encoding.UTF8.GetBytes(s); //rent from array pool here\n}\n```\n\n\n方法二：\n\n```\npublic static byte[] SerializeJson2(DrawDescriptionLayer layer)\n{\n    using (var ms = new MemoryStream())\n    using (StreamWriter writer = new StreamWriter(ms, Encoding.UTF8))\n    using (JsonTextWriter jsonWriter = new JsonTextWriter(writer))\n    {\n        JsonSerializer ser = JsonSerializer.Create(js);\n        ser.Serialize(jsonWriter, layer);\n        jsonWriter.Flush();\n        return ms.ToArray(); //rent from array pool here\n    }\n}\n```\n\n以上两种方法都无一例外的产生了额外的内存分配，效率不高。以方法二为例，创建一个新的streamwriter就会导致一个bytes[3075]的allocation，很有可能我们并不需要这么多，同理UTF8.GetBytes也有这个问题。\n\n而Utf8json则做到了按需分配，比如，当我们执行如下代码：\n\n```\nvar bytes = Utf8Json.JsonSerializer.Serialize(obj1, jsonresolver);\n```\n\nutf8json首先会从内存池取出一个buffer（每个线程一个副本）来生成bytes数组的具体内容\n\n\n```\npublic static byte[] Serialize<T>(T value, IJsonFormatterResolver resolver)\n        {\n            if (resolver == null) resolver = DefaultResolver;\n\n            var writer = new JsonWriter(MemoryPool.GetBuffer());\n            var formatter = resolver.GetFormatterWithVerify<T>();\n            formatter.Serialize(ref writer, value, resolver);\n            return writer.ToUtf8ByteArray();\n        }\n```\n然后将bytes数据拷贝的dst后返回给调用端\n\n```\nbyte[] FastCloneWithResize(byte[] src, int newSize)\n        {\n            if (newSize < 0) throw new ArgumentOutOfRangeException(\"newSize\");\n            if (src.Length < newSize) throw new ArgumentException(\"length < newSize\");\n\n            if (src == null) return new byte[newSize];\n\n            byte[] dst = new byte[newSize];\n\n#if NETSTANDARD && !NET45\n            fixed (byte* pSrc = &src[0])\n            fixed (byte* pDst = &dst[0])\n            {\n                Buffer.MemoryCopy(pSrc, pDst, dst.Length, newSize);\n            }\n#else\n            Buffer.BlockCopy(src, 0, dst, 0, newSize);\n#endif\n\n            return dst;\n        }\n```\n这样依赖内存可以复用，并且没有带来额外的损耗\n\n2.对数值型到字符串的转化进行了特定的优化，如int->string（itoa）,double->string(google/double-conversion)\n\n\n3.[优化了Buffer.MemoryCopy的性能问题](https://github.com/dotnet/coreclr/pull/3118)\n\n\n等等。\n\n\n\n## 重构\n\n定义一个Formatter专门用于处理服务端的response：\n\n\n```\n/// <summary>\n    ///  Utf8Json的JRaw,压榨性能\n    /// </summary>\n    public class JRawFormatter : IJsonFormatter<JRaw>\n    {\n        public static JRawFormatter Instance;\n\n        static JRawFormatter()\n        {\n            Instance = new JRawFormatter();\n        }\n\n\n        /// <summary>\n        ///  没有反序列化场景，暂时不管\n        /// </summary>\n        /// <param name=\"reader\"></param>\n        /// <param name=\"formatterResolver\"></param>\n        /// <returns></returns>\n        public JRaw Deserialize(ref JsonReader reader, IJsonFormatterResolver formatterResolver)\n        {\n            throw new System.NotImplementedException();\n        }\n\n        /// <summary>\n        ///  \n        /// </summary>\n        /// <param name=\"writer\"></param>\n        /// <param name=\"value\"></param>\n        /// <param name=\"formatterResolver\"></param>\n        public void Serialize(ref JsonWriter writer, JRaw value, IJsonFormatterResolver formatterResolver)\n        {\n            if (value == null)\n            {\n                writer.WriteNull();\n                return;\n            }\n\n            writer.WriteRaw(value.Payload);\n\n        }\n    }\n```\n处理服务端返回：\n\n\n```\npublic override async Task SetResponseOnHttpContextBody(HttpContext httpContext, DownstreamResponse response)\n        {\n            object body = default;\n\n            if (response.IsEmptyResponse())\n            {\n                body = new Response(null, 0);\n                httpContext.Response.StatusCode = (int)HttpStatusCode.OK;\n            }\n            else\n            {\n                var responsePayload = await response.Content.ReadAsStringAsync();\n                body = new Response(new JRaw(responsePayload));\n            }\n\n\n            var bodyBytes = Encoding.UTF8.GetBytes(JsonConvert.SerializeObject(body, _settings));\n            httpContext.Response.Headers.AddOrReplaceHeaderKey(\"Content-Length\", bodyBytes.Length.ToString());\n            await httpContext.Response.Body.WriteAsync(bodyBytes);\n        }\n```\n\n\n## 参考\n\n\n[Utf8Json](https://github.com/neuecc/Utf8Json)\n\n[The Battle of C# to JSON Serializers in .NET Core 3](https://michaelscodingspot.com/the-battle-of-c-to-json-serializers-in-net-core-3/)\n\n[Re-use memory from String to byte array conversion with ArrayPool in C#?](https://stackoverflow.com/questions/56379096/re-use-memory-from-string-to-byte-array-conversion-with-arraypool-in-c)"},{"title":"Async/Await是如何导致死锁的","url":"/2021/04/03/Await&Await是如何导致死锁的/","content":"\n## 背景\n在使用Async/Await的过程中，经常出现原因不明的死锁，感觉代码写的没啥问题，但是程序就是卡住了，为了明白这个问题，必须知道async/await的相关原理\n\n\n## 当程序遇到await关键字后会怎么处理\n\n当程序遇到await的关键字后，会做以下事件：\n\n1. 将await关键字后面的相关操作注册为一个回调\n2. 释放当前线程T\n3. 如果SynchronizationContext.Current不为null，那么在步骤1的回调将通过SynchronizationContext.Post去执行（最终是步骤2的线程T去执行这段代码）\n4. 如果SynchronizationContext.Current为null，那么在步骤1的回调将会通过线程池里面的线程去执行\n\n\n\n### 为什么会产生死锁\n如下图所示：\n[![cZt6sJ.png](https://z3.ax1x.com/2021/04/02/cZt6sJ.png)](https://imgtu.com/i/cZt6sJ)\n\n1. 在UI线程我们调用了Wait()或者.Result，这个时候UI线程会等待异步结果\n2. I/O线程异步操作结束后，根据以上描述，I/O线程需要SynchronizationContext.Post来继续执行await后面的逻辑，谁来执行？必然是UI线程，但是UI线程仍然在等待异步的结果，这个时候I/O线程和UI线程会出现相互等待导致死锁产生\n\n\n\n### 如何避免死锁\n\n如下图所示：\n\n[![cmL6oD.md.png](https://z3.ax1x.com/2021/04/03/cmL6oD.md.png)](https://imgtu.com/i/cmL6oD)\n\n使用``ConfigureAwait ``,使用ConfigureAwait，回调会通过线程池中的线程去执行，而不是SynchronizationContext关联线程(UI线程)，所以解决了这个主要矛盾，就没有死锁了\n\n\n\n### 结论\n\n由于async/await的底层使用了SynchronizationContext，SynchronizationContext有多种实现（console，gui，web）。在.NET FRAMEWORK环境下进行了实验，不仅在winform，在MVC中，在异步方法中进行同步调用，也会产生死锁。\n但是在.NET CORE中，由于取消了SynchronizationContext，在异步方法中直接进行同步调用是没有问题的，可以放心调用，但是还是不建议这样做\n\n\n\n### 参考\n[The danger of async/await and .Result in one picture](https://tooslowexception.com/the-danger-of-asyncawait-and-result-in-one-picture/)"},{"title":"使用dotnet template engine创建项目模板","url":"/2021/03/01/使用dotnet template engine创建项目模板/","content":"\n\n\n## 背景\ndotnet sdk安装后默认配套了若干开发模板，使用dotnet new 命令可以让开发者快速通过开发模板搭建项目。这些内置的开发模板基本可以满足日常的开发需求，但是对于一些特定场景，比如开发者想在Abp的基础上快速搭建项目，那么必须利用 [dotnet template engine](https://github.com/dotnet/templating)    来进行模板的定制化开发\n\n\n其实在dotnet core出来之前，比较流行的的代码自动生成的方式是利用Visual Studio内置的 [T4模板引擎](https://docs.microsoft.com/en-us/visualstudio/modeling/code-generation-and-t4-text-templates?view=vs-2019)  如下：\n\n\n\n```\n<#@ template debug=\"true\" hostSpecific=\"true\" #>\n<#@ output extension=\".cs\" #>\n<#@ Assembly Name=\"System.Core\" #>\n<#@ Assembly Name=\"System.Windows.Forms\" #>\n<#@ assembly name=\"EnvDTE\" #>  \n<#@ import namespace=\"EnvDTE\" #>  \n<#@ import namespace=\"System\" #>\n<#@ import namespace=\"System.IO\" #>\n<#@ import namespace=\"System.Diagnostics\" #>\n<#@ import namespace=\"System.Linq\" #>\n<#@ import namespace=\"System.Collections\" #>\n<#@ import namespace=\"System.Collections.Generic\" #> \n<#//修改using #>\nusing AAM.ProductManagement.Core.Models.ProductManagement;\nusing AAM.ProductManagement.Core.IRepository;\nusing AAM.Repository.EntityFramework;\n<#//修改命名空间#>\nnamespace AAM.ProductManagement.Repository\n{\n<#\n   // insert your template code here the template code will be syntax highlighted \n   // and you will have intellisense for all namespaces in the full edition\n   string solutionsPath = Host.ResolveAssemblyReference(\"$(SolutionDir)\");  \n   //修改此处路径即可生成实体\n   string modelDir=solutionsPath+\"\\\\AAM.ProductManagement.Core\\\\Models\\\\ProductManagement\\\\\";\n   string [] filesPaths=Directory.GetFiles(modelDir,\"*\",SearchOption.AllDirectories);\n\n    foreach (var item in filesPaths)\n    {\n\t\tFileInfo file=new FileInfo(item);\n\t\tstring className=file.Name.Replace(\".cs\",\"\");\n\t\tif(className.EndsWith(\"Context\")) continue;\n\t\t#>\n\n\n\tpublic partial class <#= className #>Repository: RepositoryBase<<#= className #>>, I<#= className #>Repository\n\t{\n\t\tpublic <#= className #>Repository(IProductManagementUnitOfWork unitOfWork):base(unitOfWork)\n\t\t{\n\n\t\t}\n\t}\n\t\t<#\n\t}\n#>\n}\n```\n\n这是一个用于快速生成仓储代码的模板。模板代码看起来相当复杂，如果不熟悉T4的语法，基本上是无法看懂，并且这种模板是无法直接调试的，你压根不知道你写的有没有问题，只有把整个模板渲染出来后才再编译一下才知道有没有问题。\n\n总结一下，2个问题：\n1. 调试不方便\n2. 有一定维护成本及学习成本\n\n为了解决上述问题，dotnet template engine用了另外一种思路：\n\n**模板即代码**\n\n你开发的模板首先必须是一个可以编译运行的代码，其次他才是一个模板。\n\n\n## 模板配置\n\n### 前置工作\n每一个模板项目的根目录都必须有一个.template.config目录,再这个目录下面必须有一个template.json配置问题，如下：\n\n\n| Member            | Type          | Description |\n| ----------------- | ------------- | ----------- |\n| `$schema`         | URI           | The JSON schema for the *template.json* file. Editors that support JSON schemas enable JSON-editing features when the schema is specified. For example, [Visual Studio Code](https://code.visualstudio.com/) requires this member to enable IntelliSense. Use a value of `http://json.schemastore.org/template`. |\n| `author`          | string        | The author of the template. |\n| `classifications` | array(string) | Zero or more characteristics of the template that a user might use to find the template when searching for it. The classifications also appear in the *Tags* column when it appears in a list of templates produced by using the `dotnet new -l|--list` command. |\n| `identity`        | string        | A unique name for this template. |\n| `name`            | string        | The name for the template that users should see. |\n| `shortName`       | string        | A default shorthand name for selecting the template that applies to environments where the template name is specified by the user, not selected via a GUI. For example, the short name is useful when using templates from a command prompt with CLI commands. |\n| `sourceName`       | string        | The name in the source tree to replace with the name the user specifies. The template engine will look for any occurrence of the `sourceName` mentioned in the config file and replace it in file names and file contents. The value to be replaced with can be given using the `-n` or `--name` options while running a template. If no name is specified, the current directory is used.|\n| `preferNameDirectory`       | boolean        | Indicates whether to create a directory for the template if name is specified but an output directory is not set (instead of creating the content directly in the current directory). The default value is false.|\n\n这个是template.json配置中各个字段的简要说明，下面会对几个关键字段进行介绍\n\n### sourceName\n\n如果将sourceName定义为AAA，那么在这个模板中所有以打头AAA文件名或包含AAA文本内容都会被自动替换掉，如\n\n```\ndotnet new -n BBB\n```\n那么 AAA会被替换为BBB，相关文件也会被重命名。一般来讲，sourceName的值就是模板文件中的csproj文件名或者sln文件名\n\n\n### preferNameDirectory\n这个值一般会配置为true，eg：如果我在CCC目录下执行dotnet new命令，相当于 dotnet new  -n CCC\n\n\n### symbol 特殊变量替换\n上述两点并没有彻底解决变量替换的问题，我们需要让一些我们自己定义的变量也能够被替换掉，这个时候需要使用symbol配置,\n\n```\n\"symbols\": {\n        \"protoPackage\": {\n            \"type\": \"parameter\",\n            \"replaces\": \"_ProtoPackage_\",\n            \"datatype\": \"text\",\n            \"isRequired\": true,\n            \"description\": \"Provide the proto package name\",\n            \"FileRename\": \"_ProtoPackage_\"\n        },\n        \"protoService\": {\n            \"type\": \"parameter\",\n            \"replaces\": \"_ProtoService_\",\n            \"datatype\": \"text\",\n            \"isRequired\": true,\n            \"description\": \"Provide the proto service name\",\n            \"FileRename\": \"_ProtoService_\"\n        }\n    }\n```\n\n\n```\nsyntax = \"proto3\";\n\noption csharp_namespace = \"AAA\";\n\npackage _ProtoPackage_;\n\n// The service definition.\nservice _ProtoService_ {\n}\n```\n\n如上所示，通过定义symbol变量可以将grpc proto文件的报名和服务名替换掉\n\n### 小结\n只用上述的三个配置参数，个人可以满足90%的场景需求，模板定制，无非就是变量替换，对于一些其他的场景微软官方也提供了相应的[例子](https://github.com/dotnet/dotnet-template-samples)\n\n\n\n\n\n\n\n\n## 模板发布、安装、卸载\n\n\n一般说来，模板开发完后我们会讲模板存放到内部的nuget服务器，为此，新建另外一个新的项目，来对模板文件打包，注意这里我们新建一个新的项目仅仅是为了将我们的模板内容传到nuget服务器，这个新建项目不是模板。\neg，项目结构如下：\n```\nproject_folder\n│   MyDotnetTemplates.csproj\n│\n└───templates\n    ├───mytemplate1\n    │   │   console.cs\n    │   │   readme.txt\n    │   │\n    │   └───.template.config\n    │           template.json\n    │\n    └───mytemplate2\n        │   otherfile.cs\n        │\n        └───.template.config\n                template.json\n```\n\nMyDotnetTemplates.csproj内容如下：\n```\n<Project Sdk=\"Microsoft.NET.Sdk\">\n\n  <PropertyGroup>\n    <PackageType>Template</PackageType>\n    <PackageVersion>1.0</PackageVersion>\n    <PackageId>AdatumCorporation.Utility.Templates</PackageId>\n    <Title>AdatumCorporation Templates</Title>\n    <Authors>Me</Authors>\n    <Description>Templates to use when creating an application for Adatum Corporation.</Description>\n    <PackageTags>dotnet-new;templates;contoso</PackageTags>\n    <TargetFramework>netstandard2.0</TargetFramework>\n\n    <IncludeContentInPack>true</IncludeContentInPack>\n    <IncludeBuildOutput>false</IncludeBuildOutput>\n    <ContentTargetFolders>content</ContentTargetFolders>\n  </PropertyGroup>\n\n  <ItemGroup>\n    <Content Include=\"templates\\**\\*\" Exclude=\"templates\\**\\bin\\**;templates\\**\\obj\\**\" />\n    <Compile Remove=\"**\\*\" />\n  </ItemGroup>\n\n</Project>\n```\ntemplates存发的是模板内容，用户如果执行\n```\ndotnet new -i AdatumCorporation.Utility.Template\n```\n那么就会获得模板，\n同理,通过以下命令可以删除模板内容\n\n```\ndotnet new -u AdatumCorporation.Utility.Template\n```\n\n\n\n## 参考\n[how to create your own templates for dotnet new](https://devblogs.microsoft.com/dotnet/how-to-create-your-own-templates-for-dotnet-new/)\n\n[custom-templates](https://github.com/dotnet/docs/edit/master/docs/core/tools/custom-templates.md)"},{"title":"如何Swashbuckle可以动态的增加EndPoint","url":"/2020/12/23/如何Swashbuckle可以动态的增加EndPoint/","content":"\n## 问题背景\n\n不得不说[MMLib.SwaggerForOcelot](https://github.com/Burgyn/MMLib.SwaggerForOcelot)这个Ocelot Swagger插件是真的好用，可以很方便的集成下游服务的swagger.json到网关之中，极大的简化了客户端的调试工作。但是这个插件有一个致命的缺陷，那就是无法动态的支持下游结点。也就是说如果下游增加一个新的结点，如果你想把这个结点swagger带出来的话，那么就必须重启ocelot。\n[这个issue](https://github.com/domaindrivendev/Swashbuckle.AspNetCore/issues/1093)也提到了这个问题，作者大概的意思就是说，没办法实现，要改的东西太多了，那么究竟有没有办法实现这个动态加载swagger的功能呢？答案是有\n\n\n\n## 分析问题\n\n先回忆一下我们平时我们集成swagger的常见写法\n\n\n```\n   public void ConfigureServices(IServiceCollection services)\n   {\n        services.AddSwaggerGen(c => .....);\n   }\n    public void Configure(IApplicationBuilder app, IWebHostEnvironment env)\n    {\n          app.UseSwagger();\n          app.UseSwaggerUI(options =>\n          {\n             options.SwaggerEndpoint(\"/swagger/v1/swagger.json\", \"XXX API V1\");\n          });\n    }\n```\n\n翻一下Swashbuckle的源码：\n\n```\n   public static IApplicationBuilder UseSwaggerUI(\n            this IApplicationBuilder app,\n            Action<SwaggerUIOptions> setupAction = null)\n        {\n            var options = new SwaggerUIOptions();\n            if (setupAction != null)\n            {\n                setupAction(options);\n            }\n            else\n            {\n                options = app.ApplicationServices.GetRequiredService<IOptions<SwaggerUIOptions>>().Value;\n            }\n\n            app.UseMiddleware<SwaggerUIMiddleware>(options);\n\n            return app;\n        }\n```\n\n注意到 SwaggerUIMiddleware的构造函数：\n```\n  public class SwaggerUIMiddleware\n        public SwaggerUIMiddleware(\n            RequestDelegate next,\n            IHostingEnvironment hostingEnv,\n            ILoggerFactory loggerFactory,\n            SwaggerUIOptions options)\n        {\n            _options = options ?? new SwaggerUIOptions();\n\n            _staticFileMiddleware = CreateStaticFileMiddleware(next, hostingEnv, loggerFactory, options);\n\n            _jsonSerializerOptions = new JsonSerializerOptions();\n            _jsonSerializerOptions.PropertyNamingPolicy = JsonNamingPolicy.CamelCase;\n            _jsonSerializerOptions.IgnoreNullValues = true;\n            _jsonSerializerOptions.Converters.Add(new JsonStringEnumConverter(JsonNamingPolicy.CamelCase, false));\n        }\n\n```\n\n那么无法使用dynamic endpoint的原因很明显，因为\n```\n app.UseMiddleware<SwaggerUIMiddleware>(options);\n```\n上面这句话本质上是把这个SwaggerUIMiddleware注册成一个单例，也就是说他的构造函数只会执行一次。那么你后续取到的endpoint必然就是一个“静态”的东西，那么解决这个主要矛盾就行了\n\n\n\n\n### 解决思路\n\n这里只是阐述思路，具体的实践方法可以自己去尝试：\n\n首先我们通过一个scope configurer，每一个http请求都会触发swaggeruioptions的重新config\n\n\n```\nservices.AddScoped<IConfigureOptions<SwaggerUIOptions>, SwaggerUIOptionsConfigure>();\n```\n这样一来可以让SwaggerUIOptions动态的变化\n\n\n然后，我们必须把SwaggerUIMiddleware替换掉，也就是说，我们没必要这样写：\n```\napp.UseSwagger();\n          app.UseSwaggerUI(options =>\n          {\n             options.SwaggerEndpoint(\"/swagger/v1/swagger.json\", \"XXX API V1\");\n          });\n```\n\n我们可以开发一个新的中间件来替换掉把SwaggerUIMiddleware，eg:RenderSwaggerUI\n\n```\n\n public class RenderSwaggerUI : IMiddleware\n {\n   public RenderSwaggerUI(IWebHostEnvironment hostingEnv,\n            ILoggerFactory loggerFactory,\n            IOptionsSnapshot<SwaggerUIOptions> swaggerUIOptions,\n            IOptionsSnapshot<List<RouteOptions>> routes,\n            IOptionsSnapshot<SwaggerForOcelotUIOptions> swaggerForOcelotUIOptions,\n            IOptionsSnapshot<List<SwaggerEndPointOptions>> swaggerEndPoints,\n            IHttpClientFactory httpClientFactory,\n            ISwaggerJsonTransformer swaggerJsonTransformer, ISwaggerServiceDiscoveryProvider swaggerServiceDiscoveryProvider)\n        {\n            this.hostingEnv = hostingEnv;\n            this.loggerFactory = loggerFactory;\n            this.swaggerUIOptions = swaggerUIOptions.Value;\n            this.swaggerForOcelotUIOptions = swaggerForOcelotUIOptions.Value;\n            this.httpClientFactory = httpClientFactory;\n            this.swaggerJsonTransformer = swaggerJsonTransformer;\n            this.swaggerEndPoints = swaggerEndPoints.Value;\n            this.routes = routes.Value;\n            this.swaggerServiceDiscoveryProvider = swaggerServiceDiscoveryProvider;\n        }\n    public async Task InvokeAsync(HttpContext context, RequestDelegate next)\n    {\n\n            var swaggerUIMiddleware = new SwaggerUIMiddleware(next, this.hostingEnv, this.loggerFactory, this.swaggerUIOptions);\n            await swaggerUIMiddleware.Invoke(context);\n    }\n}\n\n```\n\n注意到这里RenderSwaggerUI实现IMiddleware，目的是让这个中间件被自动注册成一个scope中间件，每次都会执行构造函数.然后手动 new 一个 swaggerUIMiddleware并Invoke\n\n\n\n### 结论\n\nSwashbuckle的dynamic endpoint是可以实现的，只不过比较绕，这种方法不是最优解，但是可以解决问题。期待后续作者解决这个问题。\n\n\n\n\n\n### 参考\n\n[Dependency injection in ASP.NET Core](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection?view=aspnetcore-5.0)"},{"title":"rxjs中tap,map,switchmap的区别","url":"/2020/10/26/rxjs中tap,map,switchmap的区别/","content":"\n## 前言\n在前端三大框架angular,react,vue中，angular是唯一一个对rxjs进行了深度集成的框架。rxjs通过大量内置operator，极大的简化了前端对事件流的处理。\n\n对于一个.NET后端开发来说，上手rxjs不算难事，rxjs的operator都能在Linq上面找到对应的原型，\n所以说对于有志于成为全栈的.NET开发，angular是一个不错的选择。\n\n在angular开发过程中，tap,map,switchmap（经常被用来做限流）这三个operator的出场率很高，但由于这三个operator的命名很相似，经常容易搞混。这里简单介绍下这三个operator的区别\n\n\n### tap\ntap，并不会返回一个新的observable，只是对observable的stream进行预处理，如在调试过程中，我们经常会用tap把stream中的元素先打印出来，观测处理前和处理后的结果。\n\n\n```\nimport { from } from \"rxjs\";\nimport { tap } from \"rxjs/operators\";\n\nfrom([1, 2, 3])\n  .pipe(tap(item =>  /* do something */))\n  .subscribe(item => console.log(item));\n```\ntap类似于Rx.NET中的do operator\n### map\nmap，对input stream进行处理，并返回一个新的observable\n\n\n```\nimport { from } from 'rxjs';\nimport { map } from 'rxjs/operators';\n\nfrom([1, 2, 3])\n  .pipe(map((item) => item + 2))\n  .subscribe((item) => console.log(item));\n```\n\nmap类似于Rx.NET中的select operator\n\n\n\n### switchMap\n\n将一个内嵌的observable做flatten处理。\n\n```\nimport { from } from 'rxjs';\nimport { map } from 'rxjs/operators';\n\nfrom([1, 2, 3])\n  observable as return value.\n itself is a new observable now,\n  .pipe(map((item) => methodWhichReturnsObservable(item)))\n  .subscribe((item) => console.log(item));\n```\n在上述代码中如果methodWhichReturnsObservable也是返回一个observable，那么console.log(item) 打印出来的item并不是一个number，而是一个observable。\n\n为了处理这一种情况，只需将map改成switchmap\n\nswitchmap类似于Rx.NET中的selectmany\n\n\n\n```\nimport { from } from \"rxjs\";\nimport { switchMap } from \"rxjs/operators\";\n\nfrom([1, 2, 3])\n  .pipe(switchMap(item => methodWhichReturnsObservable(item))\n  .subscribe(resultItem => console.log(resultItem));\n```\n\n\n\n## 参考\n\n[Rx Operators](http://reactivex.io/documentation/operators.html)"},{"title":"Grpc Channel没有关闭而导致的内存泄露问题排查","url":"/2020/09/22/GrpcChannel没有正常关闭而导致的内存泄露问题排查/","content":"\n## 背景\n日前上线了一个用于统计统计用户活跃度的服务，上线之后，出现了内存泄漏的问题，通过lldb对程序的core dump进行分析，最终找到了泄漏的原因\n\n\n\n## 分析\n服务上线后，出现大量告警，提示服务器内存不足，grafana监控提示服务内存泄漏：\n\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/WeChat%20Screenshot_20200922084321.png)\n\n\n使用将线上的服务的内存dump下来，继续观察：\n\n```\n/usr/share/dotnet/shared/Microsoft.NETCore.App/2.1.1/createdump -u 1\n```\n\n\n\n通过lldb加载core dump文件：\n\n```\nlldb-3.9 dotnet -c /tmp/20290824_activevalue_coredump  -o \"plugin load /usr/share/dotnet/shared/Microsoft.NETCore.App/2.1.1/libsosplugin.so\"\n```\n\n\n查看堆大小：\n\n```\neeheap -gc\n```\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/WeChat%20Screenshot_20200927023535.png)\n\n\n进一步查看当前堆各个类型的对象数量\n``` dumpheap -stat ```\n\n提示有大量Grpc对象积压.\n\n进一步，对项目的各个接口进行压测,对内存进行三次采样：\n\n第一次采样77MB:\n\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/dasdasdas.png)\n\n\n\n\n第二次采样82MB:\n\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/2323fadas.png)\n\n\n\n第三次采样 86MB：\n\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/adasdasgdfasdas.png)\n\n\n\nVS内存分析工具提示GRPC内存对象一直没有得到有效的控制，数量不断上升\n在采样过程中手动进行GC，只是下降53个对象，大量channel仍然驻留内存:\n\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/dasdasfdvfsdvsdf.png)\n\n\n\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/dasdascas.png)\n\n\n\n\n根据上述采样分析，可以肯定业务代码中存在grpc channel正常关闭的场景。\n\n\n进一步查看代码发现：\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/WeChat%20Screenshot_20200927024147.png)\n\n每一次通过grpc client调用其他服务的时候会直接常见Channel，而不是复用之前的Channel，并且创建后的Channel没有关闭\n\n\n修复这部分代码后，线上内存使用率恢复正常\n\n\n\n## 参考\n\n[Channel creation best practice](https://github.com/grpc/grpc-java/issues/3268)\n\n[Performance best practices with gRPC](https://docs.microsoft.com/zh-cn/aspnet/core/grpc/performance?view=aspnetcore-5.0)！！！"},{"title":"Union查询性能优","url":"/2020/09/21/Union查询性能优/","content":"\n## Union的工作原理\n\n假设现在有两个集合A：\n```\nblue\ngreen\ngray\nblack\n```\n集合B：\n```\nred\ngreen\nyellow\nblue\n```\n现在对集合A、B进行Union操作，首先Mysql会创建一个临时表来暂存A ∪ B = C\nStep1(将A的元素和B的元素都添加到C中)\n```\nblack\nblue\nblue\ngray\ngreen\ngreen\nred\nyellow\n```\n\n接下来对C进行去重，得到：\n```\nblack\nblue\ngray\ngreen\nred\nyellow\n```\n\n## 如何优化\n\n### 使用Union All\n\n对一个大数据集进行去重操作需要很高的算力，当业务场景可以接收重复结果的情况下，使用Union All，Union All不会对结果集进行去重，这个时候可以减少Union去重带来的性能损耗\n\n\n### 将查询条件放在Union的子查询中\n\n如果不将查询条件放在子查询之中，这个时候默认Union将各个子查询的结果全部查出来，然后放到一个临时表中，再对临时表进行过滤，这个时候，没办法利用到Mysql的索引，不利于性能的提高。所以使用Union操作的时候要尽量将查询条件，排序条件放在子查询之中。\n另外需要注意的一点是，如果直接对子查询进行Order By操作，Mysql会提示```Incorrect usage of UNION and ORDER BY```,如：\n```\nSELECT * FROM t1 WHERE username LIKE 'l%' ORDER BY score ASC\nUNION\nSELECT * FROM t1 WHERE username LIKE '%m%' ORDER BY score ASC\n```\n\n解决的办法就是用一个括号将子查询包起来，再进行Union操作：\n\n```\n(SELECT * FROM t1 WHERE username LIKE 'l%' ORDER BY sroce ASC)\nUNION\n(SELECT * FROM t1 WHERE username LIKE '%m%' ORDER BY score ASC)\n```\n\n## 参考\n[MySQL中union和order by同时使用的实现方法](https://www.jb51.net/article/99842.htm)\n\n[How to Optimize MySQL UNION For High Speed](https://www.iheavy.com/2013/06/13/how-to-optimize-mysql-union-for-high-speed/)"},{"title":"ThreadLocal的应用","url":"/2020/09/14/ThreadLocal在RabbitMq中的应用/","content":"## 基本概念\n\n### Thread-Local Storage\n\n在日常开发过程中，你有时候可能会希望一些对象，变量可以做到线程隔离，不相互影响。为了实现这个需求，.NET引入了一个被称为`Thread-Local Storage` 的概念，通过`Thread-Local Storage`，变量、对象无需重复定义，也不需要添加锁，就可以实现在线程之间的相互隔离\n\n\n### ThreadLocal<T>\n\n`ThreadLocal` 是 `Thread-Local Storage`的一个具体实现，一个`ThreadLocal`的对象字段，在不通线程之中可以做到相互隔离\n\n示例代码：\n\n```\nclass Program\n    {\n        static void Main(string[] args)\n        {\n            // contruct on this thread\n            ThreadLocalDemo demoObject = new ThreadLocalDemo();\n            demoObject.PrintCounterValue();\n            demoObject.IncrementCounter();\n            demoObject.PrintCounterValue();\n\n          \n            var resetEvent = new ManualResetEvent(false);\n            ThreadPool.QueueUserWorkItem(state =>\n            {\n             \n                ((ThreadLocalDemo)state).PrintCounterValue();\n                ((ThreadLocalDemo)state).IncrementCounter();\n                ((ThreadLocalDemo)state).PrintCounterValue();\n\n              \n                var demo = new ThreadLocalDemo();\n                demo.IncrementCounter();\n                demo.PrintCounterValue();\n\n                resetEvent.Set();\n            }, demoObject);\n\n            resetEvent.WaitOne();\n        }\n    }\n\n    public class ThreadLocalDemo\n    {\n        private ThreadLocal<int> counter;\n        private static ThreadLocal<int> staticCounter;\n        private static Guid staticId;\n        private Guid id;\n\n        static ThreadLocalDemo()\n        {\n            staticCounter = new ThreadLocal<int>();\n            staticId = Guid.NewGuid();\n            Console.WriteLine($\"Static constructor:\");\n            Console.WriteLine($\"thread id: {Thread.CurrentThread.ManagedThreadId}, type id: {staticId}\");\n        }\n\n        public ThreadLocalDemo()\n        {\n            counter = new ThreadLocal<int>();\n            id = Guid.NewGuid();\n            Console.WriteLine($\"Constructor: thread id: {Thread.CurrentThread.ManagedThreadId}, object id {id}\");\n        }\n\n        public void IncrementCounter()\n        {\n            Console.WriteLine(\"Incrementing both local and static counters\");\n            counter.Value++;\n            staticCounter.Value++;\n        }\n\n        public void PrintCounterValue()\n        {\n            Console.WriteLine(\"Printing current values\");\n            Console.WriteLine($\"thread id: {Thread.CurrentThread.ManagedThreadId}, object id {id}, counter: {counter.Value}, static Counter: {staticCounter.Value}\");\n        }\n    }\n```\n\n输出结果：\n\n```\nStatic constructor:\nthread id: 1, type id: 220a1d54-0d32-4961-9f19-9da5a625eaeb\nConstructor: thread id: 1, object id a655a69c-4c16-4a6e-88b3-c339f247d895\nPrinting current values\nthread id: 1, object id a655a69c-4c16-4a6e-88b3-c339f247d895, counter: 0, static Counter: 0\nIncrementing both local and static counters\nPrinting current values\nthread id: 1, object id a655a69c-4c16-4a6e-88b3-c339f247d895, counter: 1, static Counter: 1\nPrinting current values\nthread id: 4, object id a655a69c-4c16-4a6e-88b3-c339f247d895, counter: 0, static Counter: 0\nIncrementing both local and static counters\nPrinting current values\nthread id: 4, object id a655a69c-4c16-4a6e-88b3-c339f247d895, counter: 1, static Counter: 1\nConstructor: thread id: 4, object id 3175a74e-255c-4934-ae9f-e9790d05e486\nIncrementing both local and static counters\nPrinting current values\nthread id: 4, object id 3175a74e-255c-4934-ae9f-e9790d05e486, counter: 1, static Counter: 2\n```\n\n\n上述代码，所在demoObj的实例计数器在Thread 1 为 1，但当进入到Thread 4 的时候，实例计数器被重置为0，也就是说`counter`的值受线程影响，不同的线程有不同的拷贝\n\n## 具体应用\n\n\n`ThreadLocal`目前在自己维护的项目中应用的比较少，大多数情况会选择在线程上下文中定义一个新的变量，而不是去引用外部的`ThreadLocal`变量，目前唯一用到的就是在RabbitMQ。\n\n根据RabbitMQ的官方文档，IModel对象不是一个线程安全的对象，每一个每一个线程应该有且只有一个IModel,建多了也没意义：\n\n\n```\nDon’t share channels between threads\nUse one channel per thread in your application, and make sure that you don’t share channels between threads as most clients don’t make channels thread-safe.\n\nCloudAMQP allows you to scale your instances to meet demand while providing mechanisms to troubleshoot leaks. If you have any questions, you can reach out to us at support@cloudamqp.com\n```\n\n\n```\n  public interface IMessagePublisher\n    {\n        Task PublishAsync<TMessage>(TMessage message);\n    }\n\n    public class MessagePublisher : IMessagePublisher\n    {\n\n        private readonly ISerializer _serializer;\n        public MessagePublisher(ISerializer serializer, IConnectionFactory connectionFactory)\n        {\n            _serializer = serializer;\n            _channel = new ThreadLocal<IModel>(() => connectionFactory.CreateConnection().CreateModel());\n        }\n\n        public async Task PublishAsync<TMessage>(TMessage message)\n        {\n            var type = typeof(TMessage);\n            var exchangeName = GetExchangeName(type);\n            var routingKey = GetRoutingKey(type);\n            var data = _serializer.Serialize(message);\n\n            await PublishAsync(exchangeName, routingKey, data);\n        }\n    }\n    \n     services.AddSingleton<IMessagePublisher, MessagePublisher>();\n```\n\n通过上述代码，虽然IMessagePublisher是单例，但channel也可以实现做到每一个线程唯一，最大程度的减少了对象的创建\n\n\n## 参考\n\n[What is the relationship between connections and channels in RabbitMQ?](https://www.cloudamqp.com/blog/2019-11-13-the-relationship-between-connections-and-channels-in-rabbitmq.html)"},{"title":"基于 kubernetes 的动态 jenkins slave的自动化发布","url":"/2020/08/10/基于 kubernetes 的动态 jenkins slave的自动化发布/","content":"\n# 背景\n前几天在youtube上面看了一个关于如何[使用jenkins实现k8s的CI/CD](https://www.youtube.com/watch?v=eMOzF_xAm7w&t=3s),现对视频中相关的配置方法进行了文字性的总结，并在此基础上引入了sematic release实现版本号的自动生成\n\n## jenkins 动态 slave\n持续构建与发布是我们日常工作中必不可少的一个步骤，目前大多公司都采用 Jenkins 集群来搭建符合需求的 CI/CD 流程，然而传统的 Jenkins Slave 一主多从方式会存在一些痛点，比如：\n\n主 Master 发生单点故障时，整个流程都不可用了\n每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲\n资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态\n资源有浪费，每台 Slave 可能是物理机或者虚拟机，当 Slave 处于空闲状态时，也不会完全释放掉资源。\n正因为上面的这些种种痛点，我们渴望一种更高效更可靠的方式来完成这个 CI/CD 流程，而 Docker 虚拟化容器技术能很好的解决这个痛点，又特别是在 Kubernetes 集群环境下面能够更好来解决上面的问题，下图是基于 Kubernetes 搭建 Jenkins 集群的简单示意图：\n![image](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/clipboard.png)\n\n## helm\nKubernetes是容器集群管理系统，每个成功的软件平台都有一个优秀的打包系统，比如 Debian、Ubuntu 的 apt，Redhat、Centos 的 yum。而 Helm 则是 Kubernetes 上的包管理器。helm相当于一个应用商店，将一个在云上部署的应用相关的组件全部打包起来，进行安装、升级、管理等，如下图所示：\n![image](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/clipboard%20(1).png)\n\n### HelmClient \n是用户命令行工具，其主要负责如下：\n- 本地 chart 开发\n- 仓库管理\n- 与 Tiller sever 交互\n- 发送预安装的 chart\n- 查询 release 信息\n- 要求升级或卸载已存在的 release\n\n### TillerServer\n是一个部署在 Kubernetes集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下：\n- 监听来自 Helm client 的请求\n- 通过 chart 及其配置构建一次发布\n- 安装 chart 到 Kubernetes集群，并跟踪随后的发布\n- 通过与 Kubernetes交互升级或卸载 chart\n- 简单的说，client 管理 charts，而 server 管理发布 release\n\n\n## Semantic Release\n是一个自动生成版本号,发布日志的工具。semantic release按照\n[Semantic Versioning](https://semver.org/)的规范,根据用户的[commit message](https://github.com/angular/angular/blob/master/CONTRIBUTING.md)来确定下一个版本号\n\n# 配置\n\n## Jenkins\n\n### 安装Kubernetes插件\n\n![image](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/clipboard%20(2).png)\n\n### 配置kubernetes插件\n![image](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/clipboard%20(3).png)\n\n1.Kubernetes URL，API SERVER的URL\n\n2.Kubernetes server certificate key，证书密钥,可以在 ~/.kube/config 文件中获得\n\n3.Kubernetes Namespace，Slave Pod所在的Kubernetes namespace\n\n4.Credentials，访问API SERVER时使用的Token\n\n5.Jenkins URL，Jenkins Master节点的URL\n\n6.Jenkins Tunnel，Jenkins Tunnel地址\n\n7.授权,namespace与3保持一致\n\n```\n\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: jenkins\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: jenkins\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\"]\n  verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]\n  verbs: [\"get\",\"list\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: jenkins\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: jenkins\nsubjects:\n- kind: ServiceAccount\n  name: jenkins\n```\n\n```\nkubectl create clusterrolebinding jenkins --clusterrole cluster-admin --serviceaccount=<namespace>:jenkins\n```\n\n\n\n\n\n# 发布配置\n\n## 目录结构\n![image](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/clipboard%20(4).png)\n\n1. charts目录，存放helm 模板文件\n2. src，项目代码\n3. .releaserc.json，semantci-release配置文件\n4. CHANGELOG.MD，发布日志，由semantic-release自动生成\n5. Jenkinsfile，Jenkins 流水线的定义文件\n\n## gitlab 集成\n\n![image](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/clipboard%20(5).png)\n\n## 创建Pipeline任务\n\n\n### 配置pipeline环境变量\n\n![image](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/clipboard%20(6).png)\n\n\n### 配置gitlab trigger\n\n![image](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/clipboard%20(7).png)\n\n注意：需要勾选**ci-skip**选项,防止自动生成changelog时重复触发CI\n\n\n### 配置Pipeline\n\n![image](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/clipboard%20(8).png)\n\n\n\n\n\n##  集成semantic-release\n\n```\n{\n    \"branch\": \"master\",\n    \"plugins\": [\n        \"@semantic-release/commit-analyzer\",\n        \"@semantic-release/release-notes-generator\",\n        [\n            \"@semantic-release/changelog\",\n            {\n                \"changelogFile\": \"CHANGELOG.md\",\n                \"changelogTitle\": \"# Changelog | Consul Kube Sync\"\n            }\n        ],\n        [\n            \"@semantic-release/git\",\n            {\n                \"assets\": [\n                    \"CHANGELOG.md\"\n                ],\n                \"message\": \"[ci-skip]\"\n            }\n        ],\n        [\n            \"@semantic-release/exec\",\n            {\n                \"prepareCmd\": \"echo ${nextRelease.version} > .next-version\"\n            }\n        ]\n    ]\n}\n\n``` \n\n##  jenkinsfile\n\n请参考备注示例项目\n\n\n\n# 备注\n\n[示例项目](https://github.com/LoremipsumSharp/JenkinsKubernetesAutomation)\n\n\n\n\n\n\n\n"},{"title":"DNS的一些入门的基本概念","url":"/2020/08/05/DNS的一些入门的基本概念/","content":"\n## Authoritative Servers\nDNS Servers can be configured to host more than one domain. A server can be primary for one domain, and secondary for another. The term authoritative refers to any DNS servers that has a complete copy of the domain's information, whether it was entered by an administrator or transferred from a primary server. Thus, a secondary server can and should be authoritative for any domain for which it performs secondary authoritative resolution.\n## Authoritative Responses\nAny response to a DNS query that originates from a DNS server with a complete copy of the zone file is said to be an 'authoritative response'. What complicates matters is that DNS servers cache the answers they receive. If a DNS server has an SOA record, it fills in a field in the response that signals that the server queried is authoritative for the domain and that the answer is authoritative. Any DNS server external to that domain that retrieved the authoritative response will cache that answer. The next time the server is queried, it will say that the answer it is giving is authoritative, even though it is not authoritative for that domain.\n\nIn other words, it IS possible for a DNS server that is NOT an authoritative server for a domain to give an 'authoritative response' to a DNS query for a domain it does not serve.\n\nNon-authoritative responses come from DNS servers that have cached an answer for a given host, but received that information from a server that is not authoritative for the domain.\n\n\n## Kind of DNS Server\nDNS architecture works on an inverted tree structure. At the top of the inverted tree is the 13 DNS root servers, and then comes the TLD(Top Level Domain) servers, and beneath the TLD servers comes the authoritative DNS server for a particular domain(sometimes called as secondary domains.)\n\n\n## Kind of DNS Query\n[difference between iterative and recursive dns query](https://www.slashroot.in/difference-between-iterative-and-recursive-dns-query)\n\n\n## Dns Zone\n[What is a DNS ZONE file: A Complete Tutorial on zone file and its contents](https://www.slashroot.in/what-dns-zone-file-complete-tutorial-zone-file-and-its-contents)"},{"title":".NET内存管理中需要掌握的一些基本概念(持续更新）","url":"/2020/07/16/内存管理中需要掌握的一些基本概念/","content":"## Working set\nthis is a part of virtual address space that currently resides in the physical memory. This means it can be further divided into:\n- Private working set - consists of committed (private) pages in the\nphysical memory.\n- Shareable working set - consists of all shareable pages (no matter if they are actually shared or not)\n- Shared working set - consists of shareable pages that are actually\nshared with other processes.\n- Private bytes - all committed (private) pages - both in the physical and paged memory.\n- Virtual bytes - both committed (private) and reserved memory\n- Paged bytes - part of the virtual bytes that are stored in the page file.\n ![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/WeChat%20Screenshot_20200716021336.png)\n\n## Value Types\n\nWe have two categories of value types in the Common Language Specification:\n\n- structs - there are many built-in integral types (char, byte, integer, and so forth), floating-point types, and bool. And, of course, the user can define its own structs.\n- enumerations - they are basically an extension of integral types, becoming a type that consists of a set of named constants. From the memory-management point of view, they are just integral types so we won’t deal with them in this book at all as they are in fact structs also internally.\n\n\n## 使用struct的好处\n\n1. struct类型是Value Type，所以struct有可能被分配在stack上，分配在stack上可以避免因为分配的heap上所带来的gc\n2. strcut在同等条件下，所需的内存比class要小（这个是因为struct不需要存放相关的元信息）\n3. struct更加符合data locality的要求,refernce type 总是包含两个额外的字段，给定一个64B的cache line,refernce type命中缓存的概率远低于strcut\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/WeChat%20Screenshot_20200716025533.png)\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/WeChat%20Screenshot_20200716025454.png)\n\n## Lexical Scope\nIn the simplest words,it defines areas of code in which the given variable is visiable"},{"title":"在Kafka流处理中自定义时间窗口","url":"/2020/07/05/Kafka流处理中自定义时间窗口/","content":"## 问题的背景\n近期有一个项目在做重构，一部分业务要迁移到Kafka上面来进一步提高性能，需求是这样的：根据一定规则量化用户行为并实时统计用户活跃度，每一个统计周期1天，即24小时。\n\n咋一看，这个问题很好处理，直接使用Kafka流处理中的时间窗口函数，每一个窗口的长度为24小时，每次接收到一条消息就通过Kafka Stream Api进行聚合，时间窗口可以直接使用Tumbling Windows来解决。但是产品还提出了另外一个需求，就是统计时间范围，从每天的22:00 ~ 第二天的21:59\n\n这种情况，无法用Tumbling windows来解决，因为Tumbling windows定义的时间范围是从每天的（00:00~23:59)，Kafka的相关文档也没又提到这个场景需要如何处理。\n\n\n## 实现方法\n实际上，这个需求可以通过一下两种方法：\n1. 通过Timeextractor来重写时间\n2. 自定义时间窗口\n\n\n### 通过TimeExtractor来重写时间戳\n\n既然产品需求是从每天的22:00 ~ 第二天的21:59，那么我可以在原有的时间戳的基础上加两个小时来补全Tumbling windows所定义的时间范围\n\n```\npublic class UserEventTimestampExtractor implements TimestampExtractor {\n\n    @Override\n    public long extract(ConsumerRecord<Object, Object> record, long partitionTime) {\n        UserEvent userEvent = (UserEvent) record.value();\n        return userEvent.getCreateTime().plusHours(2)\n        .toInstant(ZoneOffset.UTC).toEpochMilli();\n    }\n\n}\n```\n\n### 自定义时间窗口（建议使用，更加直观）\n\n继承 Windows<TimeWindow> 并重写 windowsFor 方法\n\n\n```\n    @Override\n    public Map<Long, TimeWindow> windowsFor(final long timestamp) {\n        final Instant instant = Instant.ofEpochMilli(timestamp);\n\n        final ZonedDateTime zonedDateTime = instant.atZone(zoneId);\n        final ZonedDateTime startTime = zonedDateTime.getHour() >= startHour ? zonedDateTime.truncatedTo(ChronoUnit.DAYS).withHour(startHour) : zonedDateTime.truncatedTo(ChronoUnit.DAYS).minusDays(1).withHour(startHour);\n        final ZonedDateTime endTime = startTime.plusDays(1);\n\n        final Map<Long, TimeWindow> windows = new LinkedHashMap<>();\n        windows.put(toEpochMilli(startTime), new TimeWindow(toEpochMilli(startTime), toEpochMilli(endTime)));\n        return windows;\n    }\n```\n这里的starhour取22\n\n\n## 参考\n[kafka-streams-examples](https://github.com/confluentinc/kafka-streams-examples/blob/5.5.0-post/src/test/java/io/confluent/examples/streams/window/DailyTimeWindows.java)\n"},{"title":"如何安装特定版本的dotnet sdk","url":"/2020/06/25/如何安装特定版本的dotnet sdk/","content":"在ubuntu上通过package mananger安装dotnet sdk默认情况下会自动安装最新版的,但有些时候我们并不希望安装最新版，而是安装一个特定的版本，这个时候可以通过一下三种方法：\n1. 执行[dotnet-install](https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-install-script) 并指定特定版本号，如:\n\n```\n./dotnet-install.sh --version 3.1.201\n```\n\n\n2. 手动下载二进制tar包并解压（比较麻烦）\n3. 通过package mananger进行安装(官网目前没有介绍这种方法)\n\n```\n#首先查看目前可安装的包版本\napt policy dotnet-sdk-3.1\n#安装指定版本\napt-get install  dotnet-sdk-3.1=3.1.201-1\n```\n"},{"title":"Spring Data MongoDb 通过Gradle集成QureyDSL的配置方法","url":"/2020/05/17/Spring Data MongoDb 通过Gradle集成QureyDSL的配置方法/","content":"\n## 引言\nQueryDSL是一个通用的查询框架，专注于通过Java API构建类型安全的SQL查询。目前在 Github 上的发布的 Release 版本已经多达 251 个版本，目前最新版是 4.3.1 ，并且由 Querydsl Google组 和 StackOverflow 两个团队提供支持。QueryDSL 是一个框架，可用于构造静态类型的类似SQL的查询。可以通过诸如 QueryDSL 之类的 API 构造查询，而不是将查询编写为内联字符串或将其外部化为XML文件。\n\n例如，与简单字符串相比，使用 API 的好处是\n\n- IDE中的代码完成\n\n- 几乎没有语法无效的查询\n\n- 可以安全地引用域类型和属性\n\n- 更好地重构域类型的更改\n\n\n目前网上能够查阅到通过gradle集成QueryDSL的资料非常少，大部分是基于Maven，基于gradle的配置方法很多都是错的（这个是由于gradle的版本升级，新版本的gradle对QueryDSL没有做到向下兼容）\n\n\n## gradle配置方法\n\n\n\n```\nplugins {\n    id 'org.springframework.boot' version '2.2.5.RELEASE'\n    id 'io.spring.dependency-management' version '1.0.9.RELEASE'\n    id 'java'\n    id \"com.ewerk.gradle.plugins.querydsl\" version \"1.0.10\"\n\n}\n\ngroup = 'io.loremipsum'\nversion = '0.0.1-SNAPSHOT'\nsourceCompatibility = '11'\n\n\nrepositories {\n    mavenCentral()\n}\n\n\nquerydsl {\n    library = 'com.querydsl:querydsl-apt:4.1.4'\n    querydslSourcesDir = 'src/main/querydsl'\n    springDataMongo = true\n}\n\nsourceSets {\n    main {\n        java {\n            srcDirs = ['src/main/java', 'src/main/querydsl']\n        }\n    }\n}\n\n// 当gradle版本>5.0的时候需要加上这个配置\ncompileQuerydsl {\n    options.annotationProcessorPath = configurations.querydsl\n}\n\ndependencies {\n    compile 'org.springframework.boot:spring-boot-starter-data-mongodb'\n    compile 'org.springframework.boot:spring-boot-starter-web'\n\n    compile(\"com.querydsl:querydsl-core:4.1.4\")\n    compile(\"com.querydsl:querydsl-mongodb:4.1.4\")\n    compile(\"com.querydsl:querydsl-apt:4.1.4\")\n\n    compile 'org.projectlombok:lombok'\n    annotationProcessor 'org.projectlombok:lombok'\n\n    testCompile('org.springframework.boot:spring-boot-starter-test')\n    testCompile 'de.flapdoodle.embed:de.flapdoodle.embed.mongo'\n}\n```\n\n注意上述配置mongodb的依赖必须声明为compile（如果通过spring boot initializer创建项目,这里会被声明为implementation)，否则会提示：\n\n```\nAnnotation processor 'org.springframework.data.mongodb.repository.support.MongoAnnotationProcessor' not found\n```\n\n\n\n\n\n## 参考\n\n[Spring Boot （六）： 为 JPA 插上翅膀的 QueryDSL](https://www.cnblogs.com/babycomeon/p/11605809.html)\n\n[Querydsl Annotation Processor issue after upgrade to Gradle 5](https://stackoverflow.com/questions/53913274/querydsl-annotation-processor-issue-after-upgrade-to-gradle-5)\n\n[querydsl-plugin don't work in Gradle 5.0](https://github.com/ewerk/gradle-plugins/issues/108)"},{"title":"使用ValueTask减少假异步代码引起的GC","url":"/2020/04/27/使用ValueTask减少假异步代码引起的GC/","content":"## async/await代码存在的问题\nasync/await是.NET4.5引入的一个语法糖，如下所示，下面的代码首先会以同步的方法判断一个文件是否存在，如果存在，那么就会已异步的方式去读取文件的内容：\n\n```\npublic async Task<string> ReadFileAsync(string filename)\n{\n    if (!File.Exists(filename))\n        return string.Empty;\n    return await File.ReadAllTextAsync(filename);\n}\n```\n通过async/await开发人员可以编写更加简洁的异步代码，通过IL SPY我们可以观察到，编译器实际上将async/await转为为了一个StateMachine,如下所示：\n\n```\n[AsyncStateMachine(typeof(Program.<ReadFileAsync>d__14))]\npublic Task<string> ReadFileAsync(string filename)\n{\n    Program.<ReadFileAsync>d__14 <ReadFileAsync>d__;\n    <ReadFileAsync>d__.filename = filename;\n    <ReadFileAsync>d__.<>t__builder = AsyncTaskMethodBuilder<string>.\n    Create();\n    <ReadFileAsync>d__.<>1__state = -1;\n    AsyncTaskMethodBuilder<string> <>t__builder = <ReadFileAsync>d__.<>t__\n    builder;\n    <>t__builder.Start<Program.<ReadFileAsync>d__14>(ref <ReadFileAsync>d__);\n    return <ReadFileAsync>d__.<>t__builder.Task;\n}\n[CompilerGenerated]\n[StructLayout(LayoutKind.Auto)]\nprivate struct <ReadFileAsync>d__14 : IAsyncStateMachine\n{\n    void IAsyncStateMachine.MoveNext()\n    {\n        int num = this.<>1__state;\n        string result;\n        try\n        {\n            TaskAwaiter<string> awaiter;\n            if (num != 0)\n            {\n                if (!File.Exists(this.filename))\n                {\n                    result = string.Empty;\n                    goto IL_A4;\n                }\n                awaiter = File.ReadAllTextAsync(this.filename,\n                default(CancellationToken)).GetAwaiter();\n                if (!awaiter.get_IsCompleted())\n                {\n                    this.<>1__state = 0;\n                    this.<>u__1 = awaiter;\n                    this.<>t__builder.AwaitUnsafeOnCompleted<TaskAwaiter\n                    <string>, Program.<ReadFileAsync>d__14>(ref awaiter, ref\n                    this);\n                    return;\n                }\n        }\n        else\n        {\n            awaiter = this.<>u__1;\n            this.<>u__1 = default(TaskAwaiter<string>);\n            this.<>1__state = -1;\n        }   \n        result = awaiter.GetResult();\n    }\n    catch (Exception exception)\n    {\n        this.<>1__state = -2;\n        this.<>t__builder.SetException(exception);\n        return;\n    }\n\n    IL_A4:\n    this.<>1__state = -2;\n    this.<>t__builder.SetResult(result);\n    }\n}\n```\n\n注意到上面的代码，当文件不存在的时候会执行goto语句，也就是this.<>t__builder.SetResult(result)，t_builder是AsyncTaskMethodBuilder<T>的一个实例，\n我们可以看下SetResult的[实现](https://github.com/dotnet/runtime/blob/110282c71b3f7e1f91ea339953f4a0eba362a62c/src/libraries/System.Private.CoreLib/src/System/Runtime/CompilerServices/AsyncTaskMethodBuilderT.cs)\n\n```\npublic void SetResult(TResult result)\n{\n    // Get the currently stored task, which will be non-null if get_Task has already been accessed.\n    // If there isn't one, get a task and store it.\n    if (m_task is null)\n    {\n        m_task = GetTaskForResult(result);\n        Debug.Assert(m_task != null, $\"{nameof(GetTaskForResult)} should never return null\");\n    }\n    else\n    {\n        // Slow path: complete the existing task.\n        SetExistingTaskResult(m_task, result);\n    }\n}\n```\n由于此时这个时候并产生真正的异步操作，所以m_task is null 成立,查看GetTaskForResult(result)：\n\n```\n[MethodImpl(MethodImplOptions.AggressiveInlining)] \n// method looks long, but for a given TResult it results in a relatively small amount of asm\ninternal static Task<TResult> GetTaskForResult(TResult result)\n{\n    if (null != (object?)default(TResult)) // help the JIT avoid the value type branches for ref types\n    {\n        .....\n    }\n    else if (result == null) // optimized away for value types\n    {\n        return s_defaultResultTask;\n    }\n    return new Task<TResult>(result);\n    }\n}\n```\n由于string是引用类型且result不为null,所以必然会导致一个新的Task对象被创建，那么也就是说，即便不存在异步调用，也会创建一个新的Task对象，如果存在大量的假异步调用，势必会造成成大量的一代GC，从而影响程序的性能。\n\n## 问题的解决\n为了解决上述问题，.NET CORE 2.1引入了一个新的概念，ValueTask：\n\n```\npublic struct ValueTask<TResult>\n{\n\n    IValueTaskSource<TResult>\n    internal readonly object _obj;\n    internal readonly TResult _result;\n}\n```\n\n使用ValueTask改写ReadFile代码：\n\n```\npublic async ValueTask<string> ReadFileAsync(string filename)\n{\n    if (!File.Exists(filename))\n        return string.Empty;\n    return await File.ReadAllTextAsync(filename);\n}\n```\n当调用ReadFileAsync时，可以使用ValueTask.IsCompleted来判断这个调用是不是假异步调用，如下所示：\n\n```\nvar valueTask = ReadFileAsync();\nif(valueTask.IsCompleted)\n{\n    return valueTask.Result;\n}\nelse\n{\n    return await valueTask.AsTask();\n}\n```\n如果假异步调用，IsCompleted为true，这个时候返回valueTask.Result,不会产生Task对象的分配而ValueTask本身也是一个值类型，也不会产生allocation。反之，如果是真异步调用则按照原来的逻辑去执行。\n\n\n## 结论\n当应用程序对性能要求比较苛刻的时候并且存在大量假异步调用的情况下，可以考虑使用ValueTask来提高性能。"},{"title":"高并发场景下StackExchange.Redis驱动的超时问题","url":"/2020/04/25/高并发场景下StackExchange.Redis驱动的超时问题/","content":"## 问题背景\n近期部门的大量微服务在做国际化改造。为了实现国际化需求，需要有一个支撑服务用于提供用户ip信息数据。由于是支撑服务，会产生大量调用，大概每分钟2000次的调用，为了进一步提供性能，该服务会把用户的ip信息缓存到redis以避免重复调用第三方接口获取ip数据。\n上线前对该服务进行了压力测试，发现在高并发的场景下，会频繁发生\n**StackExchange.Redis.RedisTimeoutException**异常，这对于一个支撑服务来说是不可接受的。\n\n## 问题重现\n\n使用grpc压力测试工具[ghz](https://github.com/bojand/ghz)模拟高并发场景：\n\n```\n\tghz 192.168.1.44:43667 --insecure \\\n\t\t--proto ../../proto/FM.Region.Client/Region.proto \\\n\t\t--call Region.RegionSrv/GetRegionInfoDetailByIp \\\n  \t\t--concurrency 400 \\\n\t\t-n 1200 \\\n\t\t-D  ./ip.json\n\n```\n1200请求，400个并发，共3轮测试\n\n测试ip数据集如下：\n\n```\n[\n    {\n        \"IP\": \"27.220.195.252\"\n    },\n    {\n        \"IP\": \"32.213.120.200\"\n    },\n    {\n        \"IP\": \"58.88.5.142\"\n    },\n    {\n        \"IP\": \"99.107.218.202\"\n    },\n    {\n        \"IP\": \"126.179.177.51\"\n    },\n    {\n        \"IP\": \"75.179.58.40\"\n    },\n    {\n        \"IP\": \"92.243.129.145\"\n    },\n    {\n        \"IP\": \"179.240.146.239\"\n    },\n    {\n        \"IP\": \"54.99.209.135\"\n    },\n    {\n        \"IP\": \"116.125.57.197\"\n    },\n    {\n        \"IP\": \"104.243.227.11\"\n    },\n    {\n        \"IP\": \"65.175.113.237\"\n    },\n    {\n        \"IP\": \"92.160.105.28\"\n    },\n    {\n        \"IP\": \"51.189.156.232\"\n    },\n    {\n        \"IP\": \"101.136.19.162\"\n    },\n    {\n        \"IP\": \"128.78.227.70\"\n    },\n    {\n        \"IP\": \"123.139.77.54\"\n    },\n    {\n        \"IP\": \"172.234.209.119\"\n    },\n    {\n        \"IP\": \"187.172.248.233\"\n    },\n    {\n        \"IP\": \"28.8.211.1\"\n    },\n    {\n        \"IP\": \"130.96.236.81\"\n    },\n    {\n        \"IP\": \"88.162.103.72\"\n    },\n    {\n        \"IP\": \"166.2.94.121\"\n    },\n    {\n        \"IP\": \"102.106.115.156\"\n    },\n    {\n        \"IP\": \"19.148.120.200\"\n    },\n    {\n        \"IP\": \"219.240.103.98\"\n    },\n    {\n        \"IP\": \"221.17.125.56\"\n    },\n    {\n        \"IP\": \"91.236.176.82\"\n    },\n    {\n        \"IP\": \"41.237.239.70\"\n    },\n    {\n        \"IP\": \"145.55.30.213\"\n    },\n    {\n        \"IP\": \"139.135.98.132\"\n    },\n    {\n        \"IP\": \"72.143.86.138\"\n    },\n    {\n        \"IP\": \"198.225.10.195\"\n    },\n    {\n        \"IP\": \"136.234.70.30\"\n    },\n    {\n        \"IP\": \"103.89.118.202\"\n    },\n    {\n        \"IP\": \"44.250.218.13\"\n    },\n    {\n        \"IP\": \"116.207.166.76\"\n    },\n    {\n        \"IP\": \"140.238.239.80\"\n    },\n    {\n        \"IP\": \"31.158.163.164\"\n    },\n    {\n        \"IP\": \"182.215.64.241\"\n    },\n    {\n        \"IP\": \"220.197.147.157\"\n    },\n    {\n        \"IP\": \"117.254.129.148\"\n    },\n    {\n        \"IP\": \"94.184.10.88\"\n    },\n    {\n        \"IP\": \"219.61.223.175\"\n    },\n    {\n        \"IP\": \"185.172.199.161\"\n    },\n    {\n        \"IP\": \"184.69.18.249\"\n    },\n    {\n        \"IP\": \"166.64.229.72\"\n    },\n    {\n        \"IP\": \"212.98.0.204\"\n    },\n    {\n        \"IP\": \"160.59.24.87\"\n    },\n    {\n        \"IP\": \"48.195.150.66\"\n    },\n    {\n        \"IP\": \"147.186.60.20\"\n    },\n    {\n        \"IP\": \"138.19.147.96\"\n    },\n    {\n        \"IP\": \"8.43.149.29\"\n    },\n    {\n        \"IP\": \"108.94.149.179\"\n    },\n    {\n        \"IP\": \"111.177.253.182\"\n    },\n    {\n        \"IP\": \"99.160.130.179\"\n    },\n    {\n        \"IP\": \"125.194.19.83\"\n    },\n    {\n        \"IP\": \"26.100.156.127\"\n    },\n    {\n        \"IP\": \"105.18.80.126\"\n    },\n    {\n        \"IP\": \"128.141.4.89\"\n    },\n    {\n        \"IP\": \"80.20.225.251\"\n    },\n    {\n        \"IP\": \"41.253.214.98\"\n    },\n    {\n        \"IP\": \"36.5.58.33\"\n    },\n    {\n        \"IP\": \"56.52.122.254\"\n    },\n    {\n        \"IP\": \"162.240.161.83\"\n    },\n    {\n        \"IP\": \"195.221.65.187\"\n    },\n    {\n        \"IP\": \"223.215.140.121\"\n    },\n    {\n        \"IP\": \"42.254.253.187\"\n    },\n    {\n        \"IP\": \"99.236.88.173\"\n    },\n    {\n        \"IP\": \"87.49.237.85\"\n    },\n    {\n        \"IP\": \"124.230.221.226\"\n    },\n    {\n        \"IP\": \"46.22.123.116\"\n    },\n    {\n        \"IP\": \"105.85.140.56\"\n    },\n    {\n        \"IP\": \"69.167.167.233\"\n    },\n    {\n        \"IP\": \"176.44.47.123\"\n    },\n    {\n        \"IP\": \"40.225.1.63\"\n    },\n    {\n        \"IP\": \"102.209.225.101\"\n    },\n    {\n        \"IP\": \"62.173.169.38\"\n    },\n    {\n        \"IP\": \"51.133.22.72\"\n    },\n    {\n        \"IP\": \"31.129.69.30\"\n    },\n    {\n        \"IP\": \"194.133.28.78\"\n    },\n    {\n        \"IP\": \"9.243.223.78\"\n    },\n    {\n        \"IP\": \"185.107.13.97\"\n    },\n    {\n        \"IP\": \"155.131.137.219\"\n    }\n]\n```\n\n测试输出：\n\n```\n\n\n\nSummary:\n  Count:        1200\n  Total:        36.67 s\n  Slowest:      19.65 s\n  Fastest:      1.01 s\n  Average:      11.12 s\n  Requests/sec: 32.72\n\nResponse time histogram:\n  1011.354 [1]  |\n  2875.261 [8]  |∎\n  4739.167 [13] |∎\n  6603.074 [17] |∎∎\n  8466.981 [230]        |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n  10330.887 [117]       |∎∎∎∎∎∎∎∎∎∎∎\n  12194.794 [440]       |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n  14058.701 [172]       |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n  15922.608 [145]       |∎∎∎∎∎∎∎∎∎∎∎∎∎\n  17786.514 [44]        |∎∎∎∎\n  19650.421 [7] |∎\n\nLatency distribution:\n  10%% in 8.15 s\n  25%% in 9.21 s\n  50%% in 11.17 s\n  75%% in 12.50 s\n  90%% in 14.51 s\n  95%% in 15.28 s\n  99%% in 17.24 s\n\nStatus code distribution:\n  [OK]                 1194 responses\n  [DeadlineExceeded]   6 responses\n\nError distribution:\n  [5]   rpc error: code = DeadlineExceeded desc = context deadline exceeded\n  [1]   rpc error: code = DeadlineExceeded desc = Deadline Exceeded\n```\n测试输出提示请求超时，而且大量请求响应时间很不理想，查看日志发现StackExchange.Redis.RedisTimeoutException异常：\n\n\n```\nStackExchange.Redis.RedisTimeoutException: Timeout performing EVAL, inst: 5, queue: 1032, qu: 0, qs: 1032, qc: 0, wr: 0, wq: 0, in: 97489, ar: 0, clientName: , serverEndpoint: 192.168.1.44:43667, keyHashSlot: 693 (Please take a look at this article for some common client-side issues that can cause timeouts: http:\\/\\/stackexchange.github.io\\/StackExchange.Redis\\/Timeouts)\\n   at StackExchange.Redis.ConnectionMultiplexer.ExecuteSyncImpl[T](Message message, ResultProcessor`1 processor, ServerEndPoint server ...\n```\n## 问题解决\n根据[StackExchange.Redis官方文档](https://github.com/StackExchange/StackExchange.Redis/blob/master/docs/Timeouts.md)描述,在高并发场景下，当StackExchange.Redis的内部线程池无法满足并发要求的时候会去请求CLR的全局线程池，全局线程池的初始线程数量是根据CPU的核心数来确定的，当全局线程池的线程不够用的时候，会以500ms/per thread的速度往线程池里面添加新的线程，但这个速度在高并发场景下还是远远不够的，为此需要配置CLR线程池的最小线程数来满足高并发的场景。\n在程序入口添加如下代码：\n\n```\nThreadPool.SetMinThreads(512, 100) //worker最小线程数512,IOCP最小线程数100\n```\n\n\n## 问题验证\n配置最小线程数后再进行压力测试，测试输出如下：\n\n```\nSummary:\n  Count:        1200\n  Total:        2.66 s\n  Slowest:      2.15 s\n  Fastest:      1.99 ms\n  Average:      827.12 ms\n  Requests/sec: 451.92\n\nResponse time histogram:\n  1.987 [1]     |\n  216.449 [66]  |∎∎∎∎∎∎∎\n  430.911 [174] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n  645.373 [403] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n  859.835 [136] |∎∎∎∎∎∎∎∎∎∎∎∎∎\n  1074.297 [75] |∎∎∎∎∎∎∎\n  1288.759 [48] |∎∎∎∎∎\n  1503.221 [69] |∎∎∎∎∎∎∎\n  1717.683 [156]        |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎\n  1932.145 [13] |∎\n  2146.607 [59] |∎∎∎∎∎∎\n\nLatency distribution:\n  10%% in 332.77 ms\n  25%% in 445.91 ms\n  50%% in 547.70 ms\n  75%% in 1.28 s\n  90%% in 1.68 s\n  95%% in 1.91 s\n  99%% in 2.11 s\n\nStatus code distribution:\n  [OK]   1200 responses\n\n```\n超时问题消失,并且响应时间大幅度减小\n\n\n需要注意的是，如果机器配置不够（CPU跑满了)，再高并发场景下，仍然会出现超时问题。\n\n\n"},{"title":"记线上的一次Mysql死锁问题分析","url":"/2020/03/14/记线上的一次Mysql死锁问题分析/","content":"\n## 问题描述\n最近上线了叫做“F币”的功能，简单说下就是系统会根据用户每天在社区的活跃行为，计算每一个用户的活跃度，最后根据用户每天的总活跃度返还相应的“F币”给用户，用户得到F币之后可以在社区兑换不同的礼品,如下图所示：\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/BF0B1ABB-F459-4343-BE90-5177FB6583D5.png)\n\n在测试环境和仿真环境运行的好好的，但是上线之后经常接到用户反馈，需要多次点击才能完成领取。查看阿里云日志，提示数据库产生死锁，如下图所示：\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/9DF85AF7-0840-4323-9B0C-613480533A45.png)\n\n## 问题分析\n注意到这里有一排按钮，用户的每次点击，后台都会进行以下两个操作：\n1. 更新用户余额（用户表）\n2. 生成流水记录 （用户流水记录）\n\n注：用户表和流水表存在主外键关系\n\n以上的两个操作会放在同一个事务完成，并且由于Ef的SaveChanges并不会根据你代码执行的先后次序去更新数据库，当用户以很快的速度从左到右依次点击，存在以下可能：\n\nT1:事务A插入流水，由于存在外键，会对user对应的行上S锁。\n\nT2:事务B插入流水，由于存在外键，会对user对应的行上S锁。\n\nT3:事务A更新User的余额，请求行记录的X锁，被B事务在T2的S锁阻塞\n\nT4:事务B更新User的余额，请求行记录的X锁，被A事务在T1的S锁阻塞\n\n至此死锁产生。\n\n\n\n## 问题解决\n\n用了一个比较简单+暴力的方法：领取接口直接上redis分布式锁。\n\n\n## 总结与反思\n\n这个项目是使用DDD的思想进行开发的，自然而然地在ORM上面的选型使用了EntityFramework Code First,但是Code Firit在建表的时候会自做主张的在“多”端生成外键。对外键的使用，除开了一部分性能开销，就是上述的死锁问题，后续考虑把这部分重构为DbFirst。\n\n另外值得一提的是，后来对这部逻辑做了压测，发现这部分代码在Sql Server跑是没问题的，因为Sql Server在插入子表的时候不会对父表记录上锁，而Mysql会对父表上锁,所以产生了死锁，天下果然没有免费午餐！\n\n\n\n## 参考\n\n\n[DbContext SaveChanges Order of Statement Execution\n](https://stackoverflow.com/questions/7335582/dbcontext-savechanges-order-of-statement-execution)\n\n[Deadlock due to Foreign Key constraint](https://bugs.mysql.com/bug.php?id=48652)"},{"title":"AspNetCore2.1升级到3.1时CORS相关配置的变更","url":"/2020/02/27/AspNetCore2.1升级到3.1时CORS相关配置的变更/","content":"\n本周将公司的运营系统从AspNetCore2.1升级到了AspNetCore3.1,遇到了一些坑，这里记录以下\n\n## 2.1的相关CORS代码\n\n\n```\npublic IServiceProvider ConfigureServices(IServiceCollection services)\n{\n\n        ... // 省略\n        services.AddCors(option => option.AddPolicy(\"corsPolicy\", builders =>\n        {\n            builders.AllowCredentials().AllowAnyOrigin().AllowAnyHeader().AllowAnyMethod();\n        }));\n        ... // 省略\n}\n\npublic void Configure(IApplicationBuilder app, ILoggerFactory loggerFactory, IHostingEnvironment env)\n{\n    ... // 省略\n    app.UseCors(\"corsPolicy\");\n     ... // 省略\n}\n```\n\n如果这部分代码这3.1的服务中不加任何修改，启动时提示错误：\n\n\n```\nThe CORS protocol does not allow specifying a wildcard (any) origin and credentials at the same time. Configure the CORS policy by listing individual origins if credentials needs to be supported.\n```\n\n\n这是由于在2.1之后,AspNetCore出于安全考虑，做了更加严格的限制，在不AllowCredentials()与AllowAnyOrigin()。\n\n假设现在站点A存在一个恶意脚本，而站点B存在一个比较的敏感的接口（如转账）。如果站点B作为服务端使用AllowCredentials()与AllowAnyOrigin()的同源配置，此时站点A可以直接调用站点B的敏感接口并发送凭证信息（如Cookie），那么将导致用户信息被窃取。\n\n\n\n## 3.1的相关CORS代码\n\n### options1 显示声明允许跨域的origin\n\n```\nservices.AddCors(option => option.AddPolicy(\"corsPolicy\", builders =>\n            {\n                builders.WithOrigins(\"http://site.com\").AllowAnyHeader().AllowAnyMethod().AllowCredentials();\n            }));\n```\n\n\n### options2 使用 SetIsOriginAllowed（可以起到与AllowAnyOriginu一样的效果）\n\n\n```\nservices.AddCors(option => option.AddPolicy(\"corsPolicy\", builders =>\n            {\n                builders.SetIsOriginAllowed(origin=>true).AllowAnyHeader().AllowAnyMethod().AllowCredentials(); //SetIsOriginAllowed(origin=>true)允许所有origin\n            }));\n```\n\n\n"},{"title":"InnodDB中的Gap Locks与Next-key Locks","url":"/2020/02/11/InnodDB中的Gap Locks与Next-key Locks/","content":"\n为了理解Gap Locks与Next-key Locks首先必须了解InnodDB定义的四种隔离级别\n\n## InnodDB的四种隔离级别\n\n1. Read uncommitted(未授权读取、读未提交)：如果一个事务已经开始写数据，则另外一个事务则不允许同时进行写操作，但允许其他事务读此行数据。该隔离级别可以通过“排他写锁”实现。这样就避免了更新丢失，却可能出现脏读。也就是说事务B读取到了事务A未提交的数据。\n2. Read committed（授权读取、读提交）： 读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。该隔离级别避免了脏读，但是却可能出现不可重复读。事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。\n3. Repeatable read（可重复读取,MySQL默认隔离级别）： 可重复读是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，即使第二个事务对数据进行修改，第一个事务两次读到的的数据是一样的。这样就发生了在一个事务内两次读到的数据是一样的，因此称为是可重复读。读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。这样避免了不可重复读取和脏读，但是有时可能出现幻象读。（读取数据的事务）这可以通过“共享读锁”和“排他写锁”实现。\n4. Serializable（序列化）： 提供严格的事务隔离。它要求事务序列化执行，事务只能一个接着一个地执行，但不能并发执行。如果仅仅通过“行级锁”是无法实现事务序列化的，必须通过其他机制保证新插入的数据不会被刚执行查询操作的事务访问到。序列化是最高的事务隔离级别，同时代价也花费最高，性能很低，一般很少使用，在该级别下，事务顺序执行，不仅可以避免脏读、不可重复读，还避免了幻像读。 \n\n\n## Gap Locks\n如下图所示：假设存在一个索引(Key,pk),那么当innodb在一事务中中对(-∞,(9,5)]追加Gap Locks后,如果其他事务尝试在索引记录中的任意一个Gap添加记录,该事务将会被阻塞\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/WeChat%20Screenshot_20200217021548.png)\n\n## Next-key Locks\nNext-key Locks = Gap Locks + index-record lock。\n当在某一个事务中执行一个SELECT语句，Innodb会通过扫描相应的索引记录来找到生成一个ResultSet，这个时候被扫描到的索引记录都被被添加Next-key locks。\n举个例子，假设现在存在一张表T，T的主键为ID，ID的可能值是10，11，13，20。\n现在在一个事务中执行如下语句：\n\n```\nSELECT ID FROM T WHERE ID >=10 AND ID <= 20 FOR UPDATE;\n```\n\n这个时候会产生的Next-key Locks如下:\n\n```\n(-∞, 10]\n(10, 11]\n(11, 13]\n(13, 20]\n(20, +∞)\n```\n其他事务不能这个范围内将不能插入新的纪录（避免了幻读），也不能修改相应的记录（避免了不可重复读）\n\n可见Next-key Locks与Gap Locks主要是为了满足Repeatable read的一致性要求。\n\n另外值得注意的一点是，如果被扫描到的索引是一个唯一索引，且只有并且只有一笔记录，那么这个时候只会上index-record lock，因为这个时候其他事务产生了新的记录，也不会产生幻读。\n\n\n\n\n\n\n"},{"title":"Maven库配置阿里云加速","url":"/2020/01/19/Maven库配置阿里云加速/","content":"\n由于国内的网络原因,通过maven拉取相关package的时候总是特别慢，极大的影响到工作效率。所幸的是阿里云提供了maven库的镜像服务，体验了一下，速度飞快。配置方法如下：\n\n## Maven仓库配置阿里云加速\n- 确定settings.xml配置文件位置\n执行如下命令：\n\n```\nmvn -X\n```\n\n输入如下：\n\n```\n.....省略\n[DEBUG] Reading global settings from /usr/share/maven/conf/settings.xml\n[DEBUG] Reading user settings from /home/abc/.m2/settings.xml\n[DEBUG] Reading global toolchains from /usr/share/maven/conf/toolchains.xml\n[DEBUG] Reading user toolchains from /home/loremipsum/.m2/toolchains.xml\n[DEBUG] Using local repository at /home/abc/.m2/repository\n\n.....省略\n```\n\n- 修改settings.xml,修改mirrors xml结点下的相关配置\n\n```\n  <mirrors>\n    <!-- mirror\n     | Specifies a repository mirror site to use instead of a given repository. The repository that\n     | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used\n     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.\n     |\n    <mirror>\n      <id>mirrorId</id>\n      <mirrorOf>repositoryId</mirrorOf>\n      <name>Human Readable Name for this Mirror.</name>\n      <url>http://my.repository.com/repo/path</url>\n    </mirror>\n     -->\n    <mirror>\n      <id>aliyun-public</id>\n      <name>aliyun public</name>\n      <mirrorOf>public</mirrorOf>\n      <url>https://maven.aliyun.com/repository/public</url>\n    </mirror>\n\n    <mirror>\n      <id>aliyun-central</id>\n      <name>aliyun central</name>\n      <mirrorOf>central</mirrorOf>\n      <url>https://maven.aliyun.com/repository/central</url>\n    </mirror>\n\n    <mirror>\n      <id>aliyun-jcenter</id>\n      <name>aliyun jcenter</name>\n      <mirrorOf>jcenter</mirrorOf>\n      <url>https://maven.aliyun.com/repository/jcenter</url>\n    </mirror>\n\n    <mirror>\n      <id>aliyun-spring</id>\n      <name>aliyun spring</name>\n      <mirrorOf>spring</mirrorOf>\n      <url>https://maven.aliyun.com/repository/spring</url>\n    </mirror>\n\n    <mirror>\n      <id>aliyun-spring-milestones</id>\n      <name>aliyun spring milestones</name>\n      <mirrorOf>spring-milestones</mirrorOf>\n      <url>https://maven.aliyun.com/repository/spring</url>\n    </mirror>\n\n    <mirror>\n      <id>aliyun-spring-plugin</id>\n      <name>aliyun spring plugin</name>\n      <mirrorOf>spring-plugin</mirrorOf>\n      <url>https://maven.aliyun.com/repository/spring-plugin</url>\n    </mirror>\n\n    <mirror>\n      <id>aliyun-gradle-plugin</id>\n      <name>aliyun gradle plugin</name>\n      <mirrorOf>gradle-plugin</mirrorOf>\n      <url>https://maven.aliyun.com/repository/gradle-plugin</url>\n    </mirror>\n\n    <mirror>\n      <id>aliyun-google</id>\n      <name>aliyun google</name>\n      <mirrorOf>google</mirrorOf>\n      <url>https://maven.aliyun.com/repository/google</url>\n    </mirror>\n\n    <mirror>\n      <id>aliyun-grails-core</id>\n      <name>aliyun grails core</name>\n      <mirrorOf>grails-core</mirrorOf>\n      <url>https://maven.aliyun.com/repository/grails-core</url>\n    </mirror>\n</mirrors>\n\n```\n\n## 防坑：\n大部分的包阿里云的maven镜像库都有，一部分比较新的库上面是没有的，如springboot，阿里云上面的版本只是到了2.2.0，实际2.2.4都已经出来了，同步不是很及时。目前我都一些没有的库做了降级处理，后续再看看怎么处理这个问题\n"},{"title":"Kafka连接超时问题的本质","url":"/2020/01/15/Kafka连接超时问题的本质/","content":"## 问题的背景\n前段时间开发环境搭建了一套kafka环境，搭建完成后通过客户端去连接kafka时，当produer调用send的时候程序都会直接hang住，提示连接超时：\n\n```\norg.apache.kafka.common.errors.TimeoutException: \nExpiring 1 record(s) for demo-topic-0:120000 ms has passed since batch creation\n```\n接着我试了一下调用producer的partitionsFor方法，可以正常执行。同样的，当consumer执行poll方法时，程序也会直接卡死。\n\n## 问题的本质\nproducer是通过leader broker往partition写入数据,当producer希望写入数据的时候会向kafka请求当前的leader broker信息，这个时候如果kafka的leader broker的host信息如果和客户端不在同一个网段就会出现上述的超时现象，这也就可以解释为什么partitionsFor方法可以正常执行(不涉及到leader broker的通信)，但是send方法(涉及到leader broker的通信)执行失败。\n\n\n## 问题解决\n\n配置ADVERTISED_HOST\n\n\n```\nbash-4.4# vi $KAFKA_HOME/config/server.properties\n```\n\n这里我的配置是\n\n```\nadvertised.listeners=PLAINTEXT://127.0.0.1:9092\n```\n\n由于是使用docker部署，客户端和服务端不在同一台机器上，所以我需要把这个配置改为与客户端同一个网段的\n\n\n## 问题验证\n\n当ADVERTISED_HOST为PLAINTEXT://127.0.0.1:9092可以看到partitionsFor返回的leader broker信息为下图所示：\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/CD584EBE-9DDF-4bbb-AF15-7970A064979A.png)\n\n修改ADVERTISED_HOST后，返回信息如下：\n![](https://raw.githubusercontent.com/LoremipsumSharp/Images/master/img/53CA72B0-F276-4ef7-B524-18C970ED3443.png)\n\n\n没有再次出现连接超时的问题\n\n\n## 参考\n[kafka-listeners-explained](https://rmoff.net/2018/08/02/kafka-listeners-explained/)\n\n[Client and broker compatibility across Kafka versions](https://docs.cloudera.com/runtime/7.1.0/kafka-managing/topics/kafka-manage-client-broker-comp.html)"},{"title":"NetCore应用配置Auto Core Dump","url":"/2020/01/09/NetCore应用配置Auto Core Dump/","content":"\n## 问题背景\n前阵子线上用户标签服务频繁出现内存泄漏问题，早上服务运行的好好的，经常一到半夜服务就挂掉。由于事发半夜，很难加以人工干预，而早上dump出来的文件参考价值很低，迫切需要一种自动化的手段让服务在宕掉的时候能够保存完整的案发现场。事实上，.NET CORE可以支持这一需求，不过默认是不开启的，需要加以配置。\n\n## Auto Core Dump配置方法\n\n以下内容是我原封不动从dotnet/rumtime这个仓库拷贝过来的：\n\nEnvironment variables supported:\n\n- `COMPlus_DbgEnableMiniDump`: if set to \"1\", enables this core dump generation. The default is NOT to generate a dump.\n- `COMPlus_DbgMiniDumpType`: See below. Default is \"2\" MiniDumpWithPrivateReadWriteMemory.\n- `COMPlus_DbgMiniDumpName`: if set, use as the template to create the dump path and file name. The pid can be placed in the name with %d. The default is _/tmp/coredump.%d_.\n- `COMPlus_CreateDumpDiagnostics`: if set to \"1\", enables the _createdump_ utilities diagnostic messages (TRACE macro).\n\nCOMPlus_DbgMiniDumpType values:\n\n\n|Value|Minidump Enum|Description|\n|-|:----------:|----------|\n|1| MiniDumpNormal                               | Include just the information necessary to capture stack traces for all existing threads in a process. Limited GC heap memory and information. |\n|2| MiniDumpWithPrivateReadWriteMemory (default) | Includes the GC heaps and information necessary to capture stack traces for all existing threads in a process. |\n|3| MiniDumpFilterTriage                         | Include just the information necessary to capture stack traces for all existing threads in a process. Limited GC heap memory and information. |\n|4| MiniDumpWithFullMemory                       | Include all accessible memory in the process. The raw memory data is included at the end, so that the initial structures can be mapped directly without the raw memory information. This option can result in a very large file. |\n\n\n根据以上内容，可以在Dockerfile或者docker-compose文件加入以下环境变量\n\n```\nENV COMPlus_DbgEnableMiniDump=\"1\"  # enables core dump generation\nENV COMPlus_DbgMiniDumpName=\"/diagnostics/dumps/coredump_%d\" # core dump文件位置，一般是一个挂在在宿主机的目录\nENV COMPlus_DbgMiniDumpType=\"4\" # 建议选择4，也就是full dump，保存的“现场”更加完整，但文件也会非常大\n```\n\n\n## 实验\n\n在一个可以被调用的接口加入以下代码：\n\n```\nEnvironment.FailFast()\n```\n这个方法可以让应用直接挂掉,挂掉后观察相应的挂载目录有无生成dump文件\n\n\n\n## 参考\n\n[xplat-minidump-generation](https://github.com/dotnet/runtime/blob/8497763bbfa70455e6f08ed7aa345d43db1d22d7/docs/design/coreclr/botr/xplat-minidump-generation.md)"}]