---
title: 高并发场景下StackExchange.Redis驱动的超时问题
---
## 问题背景
近期部门的大量微服务在做国际化改造。为了实现国际化需求，需要有一个支撑服务用于提供用户ip信息数据。由于是支撑服务，会产生大量调用，大概每分钟2000次的调用，为了进一步提供性能，该服务会把用户的ip信息缓存到redis以避免重复调用第三方接口获取ip数据。
上线前对该服务进行了压力测试，发现在高并发的场景下，会频繁发生
**StackExchange.Redis.RedisTimeoutException**异常，这对于一个支撑服务来说是不可接受的。

## 问题重现

使用grpc压力测试工具[ghz](https://github.com/bojand/ghz)模拟高并发场景：

```
	ghz 192.168.1.44:43667 --insecure \
		--proto ../../proto/FM.Region.Client/Region.proto \
		--call Region.RegionSrv/GetRegionInfoDetailByIp \
  		--concurrency 400 \
		-n 1200 \
		-D  ./ip.json

```
1200请求，400个并发，共3轮测试

测试ip数据集如下：

```
[
    {
        "IP": "27.220.195.252"
    },
    {
        "IP": "32.213.120.200"
    },
    {
        "IP": "58.88.5.142"
    },
    {
        "IP": "99.107.218.202"
    },
    {
        "IP": "126.179.177.51"
    },
    {
        "IP": "75.179.58.40"
    },
    {
        "IP": "92.243.129.145"
    },
    {
        "IP": "179.240.146.239"
    },
    {
        "IP": "54.99.209.135"
    },
    {
        "IP": "116.125.57.197"
    },
    {
        "IP": "104.243.227.11"
    },
    {
        "IP": "65.175.113.237"
    },
    {
        "IP": "92.160.105.28"
    },
    {
        "IP": "51.189.156.232"
    },
    {
        "IP": "101.136.19.162"
    },
    {
        "IP": "128.78.227.70"
    },
    {
        "IP": "123.139.77.54"
    },
    {
        "IP": "172.234.209.119"
    },
    {
        "IP": "187.172.248.233"
    },
    {
        "IP": "28.8.211.1"
    },
    {
        "IP": "130.96.236.81"
    },
    {
        "IP": "88.162.103.72"
    },
    {
        "IP": "166.2.94.121"
    },
    {
        "IP": "102.106.115.156"
    },
    {
        "IP": "19.148.120.200"
    },
    {
        "IP": "219.240.103.98"
    },
    {
        "IP": "221.17.125.56"
    },
    {
        "IP": "91.236.176.82"
    },
    {
        "IP": "41.237.239.70"
    },
    {
        "IP": "145.55.30.213"
    },
    {
        "IP": "139.135.98.132"
    },
    {
        "IP": "72.143.86.138"
    },
    {
        "IP": "198.225.10.195"
    },
    {
        "IP": "136.234.70.30"
    },
    {
        "IP": "103.89.118.202"
    },
    {
        "IP": "44.250.218.13"
    },
    {
        "IP": "116.207.166.76"
    },
    {
        "IP": "140.238.239.80"
    },
    {
        "IP": "31.158.163.164"
    },
    {
        "IP": "182.215.64.241"
    },
    {
        "IP": "220.197.147.157"
    },
    {
        "IP": "117.254.129.148"
    },
    {
        "IP": "94.184.10.88"
    },
    {
        "IP": "219.61.223.175"
    },
    {
        "IP": "185.172.199.161"
    },
    {
        "IP": "184.69.18.249"
    },
    {
        "IP": "166.64.229.72"
    },
    {
        "IP": "212.98.0.204"
    },
    {
        "IP": "160.59.24.87"
    },
    {
        "IP": "48.195.150.66"
    },
    {
        "IP": "147.186.60.20"
    },
    {
        "IP": "138.19.147.96"
    },
    {
        "IP": "8.43.149.29"
    },
    {
        "IP": "108.94.149.179"
    },
    {
        "IP": "111.177.253.182"
    },
    {
        "IP": "99.160.130.179"
    },
    {
        "IP": "125.194.19.83"
    },
    {
        "IP": "26.100.156.127"
    },
    {
        "IP": "105.18.80.126"
    },
    {
        "IP": "128.141.4.89"
    },
    {
        "IP": "80.20.225.251"
    },
    {
        "IP": "41.253.214.98"
    },
    {
        "IP": "36.5.58.33"
    },
    {
        "IP": "56.52.122.254"
    },
    {
        "IP": "162.240.161.83"
    },
    {
        "IP": "195.221.65.187"
    },
    {
        "IP": "223.215.140.121"
    },
    {
        "IP": "42.254.253.187"
    },
    {
        "IP": "99.236.88.173"
    },
    {
        "IP": "87.49.237.85"
    },
    {
        "IP": "124.230.221.226"
    },
    {
        "IP": "46.22.123.116"
    },
    {
        "IP": "105.85.140.56"
    },
    {
        "IP": "69.167.167.233"
    },
    {
        "IP": "176.44.47.123"
    },
    {
        "IP": "40.225.1.63"
    },
    {
        "IP": "102.209.225.101"
    },
    {
        "IP": "62.173.169.38"
    },
    {
        "IP": "51.133.22.72"
    },
    {
        "IP": "31.129.69.30"
    },
    {
        "IP": "194.133.28.78"
    },
    {
        "IP": "9.243.223.78"
    },
    {
        "IP": "185.107.13.97"
    },
    {
        "IP": "155.131.137.219"
    }
]
```

测试输出：

```



Summary:
  Count:        1200
  Total:        36.67 s
  Slowest:      19.65 s
  Fastest:      1.01 s
  Average:      11.12 s
  Requests/sec: 32.72

Response time histogram:
  1011.354 [1]  |
  2875.261 [8]  |∎
  4739.167 [13] |∎
  6603.074 [17] |∎∎
  8466.981 [230]        |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
  10330.887 [117]       |∎∎∎∎∎∎∎∎∎∎∎
  12194.794 [440]       |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
  14058.701 [172]       |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
  15922.608 [145]       |∎∎∎∎∎∎∎∎∎∎∎∎∎
  17786.514 [44]        |∎∎∎∎
  19650.421 [7] |∎

Latency distribution:
  10%% in 8.15 s
  25%% in 9.21 s
  50%% in 11.17 s
  75%% in 12.50 s
  90%% in 14.51 s
  95%% in 15.28 s
  99%% in 17.24 s

Status code distribution:
  [OK]                 1194 responses
  [DeadlineExceeded]   6 responses

Error distribution:
  [5]   rpc error: code = DeadlineExceeded desc = context deadline exceeded
  [1]   rpc error: code = DeadlineExceeded desc = Deadline Exceeded
```
测试输出提示请求超时，而且大量请求响应时间很不理想，查看日志发现StackExchange.Redis.RedisTimeoutException异常：


```
StackExchange.Redis.RedisTimeoutException: Timeout performing EVAL, inst: 5, queue: 1032, qu: 0, qs: 1032, qc: 0, wr: 0, wq: 0, in: 97489, ar: 0, clientName: , serverEndpoint: 192.168.1.44:43667, keyHashSlot: 693 (Please take a look at this article for some common client-side issues that can cause timeouts: http:\/\/stackexchange.github.io\/StackExchange.Redis\/Timeouts)\n   at StackExchange.Redis.ConnectionMultiplexer.ExecuteSyncImpl[T](Message message, ResultProcessor`1 processor, ServerEndPoint server ...
```
## 问题解决
根据[StackExchange.Redis官方文档](https://github.com/StackExchange/StackExchange.Redis/blob/master/docs/Timeouts.md)描述,在高并发场景下，当StackExchange.Redis的内部线程池无法满足并发要求的时候会去请求CLR的全局线程池，全局线程池的初始线程数量是根据CPU的核心数来确定的，当全局线程池的线程不够用的时候，会以500ms/per thread的速度往线程池里面添加新的线程，但这个速度在高并发场景下还是远远不够的，为此需要配置CLR线程池的最小线程数来满足高并发的场景。
在程序入口添加如下代码：

```
ThreadPool.SetMinThreads(512, 100) //worker最小线程数512,IOCP最小线程数100
```


## 问题验证
配置最小线程数后再进行压力测试，测试输出如下：

```
Summary:
  Count:        1200
  Total:        2.66 s
  Slowest:      2.15 s
  Fastest:      1.99 ms
  Average:      827.12 ms
  Requests/sec: 451.92

Response time histogram:
  1.987 [1]     |
  216.449 [66]  |∎∎∎∎∎∎∎
  430.911 [174] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
  645.373 [403] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
  859.835 [136] |∎∎∎∎∎∎∎∎∎∎∎∎∎
  1074.297 [75] |∎∎∎∎∎∎∎
  1288.759 [48] |∎∎∎∎∎
  1503.221 [69] |∎∎∎∎∎∎∎
  1717.683 [156]        |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
  1932.145 [13] |∎
  2146.607 [59] |∎∎∎∎∎∎

Latency distribution:
  10%% in 332.77 ms
  25%% in 445.91 ms
  50%% in 547.70 ms
  75%% in 1.28 s
  90%% in 1.68 s
  95%% in 1.91 s
  99%% in 2.11 s

Status code distribution:
  [OK]   1200 responses

```
超时问题消失,并且响应时间大幅度减小


需要注意的是，如果机器配置不够（CPU跑满了)，再高并发场景下，仍然会出现超时问题。


